{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "\n",
    "TRAIN_BIO_PATH = \"data/processed/train_bio.csv\"\n",
    "VAL_BIO_PATH = \"data/processed/validation_bio.csv\"\n",
    "EMBEDDINGS_PATH = \"data/external/ru_en_aligned.pkl\"\n",
    "MODELS_DIR = \"models/iteration-1/\""
   ],
   "id": "42f8bb2e6e9c8636"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Информация: Вычисления будут производиться на устройстве: {device}\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"Информация: Random seed ({SEED}) установлен для обеспечения воспроизводимости.\")"
   ],
   "id": "2248fe2dcf80f355"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Гиперпараметры ",
   "id": "dc347489880f7b04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "WORD_EMBEDDING_DIM = 300 \n",
    "CHAR_EMBEDDING_DIM = 50  \n",
    "CHAR_HIDDEN_DIM = 50      \n",
    "\n",
    "LSTM_HIDDEN_DIM = 256\n",
    "LSTM_NUM_LAYERS = 2 \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 10 \n",
    "DROPOUT_RATE = 0.5 "
   ],
   "id": "a40eca17c0cc021f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Создание модели",
   "id": "bf9254404624ce99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CharEmbedding(nn.Module):\n",
    "    def __init__(self, char_vocab_size, embedding_dim, hidden_dim, dropout_rate=0.25):\n",
    "        super(CharEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(char_vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        print(\"Информация: Модуль CharEmbedding успешно инициализирован.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, word_len = x.size()\n",
    "        \n",
    "        x = x.view(batch_size * seq_len, word_len)\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        \n",
    "        output = lstm_out.permute(0, 2, 1)\n",
    "        output = torch.max(output, 2)[0]\n",
    "        \n",
    "        output = output.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        return self.dropout(output)"
   ],
   "id": "c53e827ffdce5bda"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BiLSTMCrfForNer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 word_vocab_size,\n",
    "                 word_embedding_dim,\n",
    "                 char_vocab_size,\n",
    "                 char_embedding_dim,\n",
    "                 char_hidden_dim,\n",
    "                 lstm_hidden_dim,\n",
    "                 num_tags,\n",
    "                 dropout_rate=0.33,\n",
    "                 padding_idx=0):\n",
    "        super(BiLSTMCrfForNer, self).__init__()\n",
    "\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=word_vocab_size,\n",
    "            embedding_dim=word_embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        self.word_embedding.weight.requires_grad = False\n",
    "\n",
    "        self.char_embedding = CharEmbedding(\n",
    "            char_vocab_size=char_vocab_size,\n",
    "            embedding_dim=char_embedding_dim,\n",
    "            hidden_dim=char_hidden_dim,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.embedding_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=word_embedding_dim + (2 * char_hidden_dim),\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if 2 > 1 else 0\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(2 * lstm_hidden_dim, num_tags)\n",
    "\n",
    "        self.crf = CRF(num_tags=num_tags, batch_first=True)\n",
    "        \n",
    "        print(\"Информация: Основная модель BiLSTMCrfForNer успешно инициализирована.\")\n",
    "\n",
    "    def forward(self, word_ids, char_ids, mask, tags=None):\n",
    "        # word_ids: (batch_size, seq_len)\n",
    "        # char_ids: (batch_size, seq_len, word_len)\n",
    "        # mask: (batch_size, seq_len)\n",
    "        # tags: (batch_size, seq_len)\n",
    "\n",
    "        word_embeds = self.word_embedding(word_ids)\n",
    "        char_embeds = self.char_embedding(char_ids)\n",
    "        \n",
    "        combined_embeds = torch.cat([word_embeds, char_embeds], dim=-1)\n",
    "        combined_embeds = self.embedding_dropout(combined_embeds)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(combined_embeds)\n",
    "        \n",
    "        emissions = self.classifier(lstm_out)\n",
    "\n",
    "        if tags is not None:\n",
    "            loss = -self.crf(emissions, tags, mask=mask.byte(), reduction='mean')\n",
    "            return loss\n",
    "        else:\n",
    "            decoded_tags = self.crf.decode(emissions, mask=mask.byte())\n",
    "            return decoded_tags"
   ],
   "id": "1a1edc3e6e787f9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Подача данных",
   "id": "2565014e528d16e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class NerDataset(Dataset):\n",
    "    def __init__(self, df_path, word2id=None, char2id=None, tag2id=None):\n",
    "        self.df = pd.read_csv(df_path, sep=\";\")\n",
    "        \n",
    "        self.df['tokens'] = self.df['tokens'].apply(ast.literal_eval)\n",
    "        self.df['tags'] = self.df['tags'].apply(ast.literal_eval)\n",
    "\n",
    "        if word2id is None:\n",
    "            self.word2id, self.id2word = self._build_vocab(self.df['tokens'])\n",
    "        else:\n",
    "            self.word2id, self.id2word = word2id, {v: k for k, v in word2id.items()}\n",
    "\n",
    "        if char2id is None:\n",
    "            all_chars = set(\"\".join([\"\".join(tokens) for tokens in self.df['tokens']]))\n",
    "            self.char2id, self.id2char = self._build_char_vocab(all_chars)\n",
    "        else:\n",
    "            self.char2id, self.id2char = char2id, {v: k for k, v in char2id.items()}\n",
    "\n",
    "        if tag2id is None:\n",
    "            self.tag2id, self.id2tag = self._build_vocab(self.df['tags'])\n",
    "        else:\n",
    "            self.tag2id, self.id2tag = tag2id, {v: k for k, v in tag2id.items()}\n",
    "            \n",
    "        print(f\"Информация: Dataset загружен. Размер: {len(self.df)} записей.\")\n",
    "        print(f\"Информация: Размер словаря слов: {len(self.word2id)}\")\n",
    "        print(f\"Информация: Размер словаря символов: {len(self.char2id)}\")\n",
    "        print(f\"Информация: Размер словаря тегов: {len(self.tag2id)}\")\n",
    "\n",
    "\n",
    "    def _build_vocab(self, data):\n",
    "        vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        for sequence in data:\n",
    "            for item in sequence:\n",
    "                if item not in vocab:\n",
    "                    vocab[item] = len(vocab)\n",
    "        id2vocab = {v: k for k, v in vocab.items()}\n",
    "        return vocab, id2vocab\n",
    "\n",
    "    def _build_char_vocab(self, chars):\n",
    "        vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        for char in sorted(list(chars)):\n",
    "            if char not in vocab:\n",
    "                vocab[char] = len(vocab)\n",
    "        id2vocab = {v: k for k, v in vocab.items()}\n",
    "        return vocab, id2vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        tokens = row['tokens']\n",
    "        tags = row['tags']\n",
    "\n",
    "        word_ids = [self.word2id.get(token, self.word2id[\"<UNK>\"]) for token in tokens]\n",
    "        tag_ids = [self.tag2id.get(tag, 0) for tag in tags] # Паддинг-тег для OOV-тегов не должен происходить\n",
    "        \n",
    "        char_ids = []\n",
    "        for token in tokens:\n",
    "            ids = [self.char2id.get(char, self.char2id[\"<UNK>\"]) for char in token]\n",
    "            char_ids.append(ids)\n",
    "\n",
    "        return {\"words\": word_ids, \"chars\": char_ids, \"tags\": tag_ids}\n",
    "\n",
    "def collate_fn(batch, word_pad_idx=0, char_pad_idx=0, tag_pad_idx=0):\n",
    "    max_seq_len = max(len(item['words']) for item in batch)\n",
    "    max_word_len = max(max(len(char_seq) for char_seq in item['chars']) if item['chars'] else 0 for item in batch)\n",
    "\n",
    "    padded_words, padded_chars, padded_tags, masks = [], [], [], []\n",
    "\n",
    "    for item in batch:\n",
    "        seq_len = len(item['words'])\n",
    "        \n",
    "        padded_words.append(item['words'] + [word_pad_idx] * (max_seq_len - seq_len))\n",
    "        padded_tags.append(item['tags'] + [tag_pad_idx] * (max_seq_len - seq_len))\n",
    "        \n",
    "        masks.append([1] * seq_len + [0] * (max_seq_len - seq_len))\n",
    "        \n",
    "        padded_char_seq = []\n",
    "        for char_seq in item['chars']:\n",
    "            padded_char_seq.append(char_seq + [char_pad_idx] * (max_word_len - len(char_seq)))\n",
    "        \n",
    "        if seq_len < max_seq_len:\n",
    "            for _ in range(max_seq_len - seq_len):\n",
    "                padded_char_seq.append([char_pad_idx] * max_word_len)\n",
    "        \n",
    "        padded_chars.append(padded_char_seq)\n",
    "\n",
    "    return {\n",
    "        \"words\": torch.tensor(padded_words, dtype=torch.long),\n",
    "        \"chars\": torch.tensor(padded_chars, dtype=torch.long),\n",
    "        \"tags\": torch.tensor(padded_tags, dtype=torch.long),\n",
    "        \"mask\": torch.tensor(masks, dtype=torch.bool)\n",
    "    }"
   ],
   "id": "1c34813e294d9b1b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Цикл обучения и оценки",
   "id": "646479da5e7ee2c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_class_weights(tags_series, tag2id):\n",
    "    all_tags = [tag for seq in tags_series for tag in seq]\n",
    "    tag_counts = Counter(all_tags)\n",
    "    \n",
    "    # Создаем веса. Используем сглаживание, чтобы избежать деления на ноль.\n",
    "    # Более редкие классы получат больший вес.\n",
    "    weights = torch.ones(len(tag2id), device=device)\n",
    "    for tag, count in tag_counts.items():\n",
    "        if tag in tag2id:\n",
    "            # Вес обратно пропорционален частоте\n",
    "            weights[tag2id[tag]] = 1.0 / (count + 1e-6) \n",
    "    \n",
    "    # Нормализуем веса\n",
    "    weights = weights / weights.sum()\n",
    "    # Увеличим вес редких классов еще сильнее\n",
    "    weights = weights.pow(0.5)\n",
    "\n",
    "    print(\"Информация: Рассчитаны веса для классов:\")\n",
    "    for tag, i in tag2id.items():\n",
    "        print(f\"- {tag}: {weights[i]:.4f}\")\n",
    "        \n",
    "    return weights\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        # Перенос данных на нужное устройство\n",
    "        words = batch['words'].to(device)\n",
    "        chars = batch['chars'].to(device)\n",
    "        tags = batch['tags'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "\n",
    "        # Обнуление градиентов\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Прямой проход и вычисление потерь\n",
    "        loss = model(words, chars, mask, tags)\n",
    "        \n",
    "        # Обратный проход\n",
    "        loss.backward()\n",
    "        \n",
    "        # Шаг оптимизатора\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def eval_epoch(model, dataloader, id2tag):\n",
    "    model.eval()\n",
    "    all_true_tags = []\n",
    "    all_pred_tags = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            words = batch['words'].to(device)\n",
    "            chars = batch['chars'].to(device)\n",
    "            tags = batch['tags'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            \n",
    "            # Получение предсказаний\n",
    "            predictions = model(words, chars, mask)\n",
    "            \n",
    "            # \"Разворачиваем\" батчи в плоские списки, убирая паддинг\n",
    "            for i in range(len(predictions)):\n",
    "                seq_len = mask[i].sum().item()\n",
    "                true_tags = tags[i][:seq_len].cpu().tolist()\n",
    "                pred_tags = predictions[i][:seq_len]\n",
    "                \n",
    "                all_true_tags.extend([id2tag[tag_id] for tag_id in true_tags])\n",
    "                all_pred_tags.extend([id2tag[tag_id] for tag_id in pred_tags])\n",
    "\n",
    "    # Игнорируем тег 'O' при расчете метрик, так как он доминирует\n",
    "    labels_to_include = [tag for tag in id2tag.values() if tag != \"O\" and tag != \"<PAD>\"]\n",
    "    \n",
    "    report = classification_report(\n",
    "        all_true_tags, \n",
    "        all_pred_tags, \n",
    "        labels=labels_to_include,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    return report"
   ],
   "id": "c47cb657604d7dd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Подготовка эмбеддингов",
   "id": "99163b9c4443e94c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_and_prepare_embeddings(word2id, filepath, embedding_dim):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        fasttext_model = pickle.load(f)\n",
    "\n",
    "    embedding_matrix = np.random.uniform(-0.05, 0.05, (len(word2id), embedding_dim))\n",
    "    \n",
    "    hits = 0\n",
    "    for word, i in word2id.items():\n",
    "        if word in fasttext_model:\n",
    "            embedding_matrix[i] = fasttext_model[word]\n",
    "            hits += 1\n",
    "    \n",
    "    print(f\"Информация: Найдено {hits} из {len(word2id)} слов в предобученной модели ({hits / len(word2id) * 100:.2f}%).\")\n",
    "    \n",
    "    embedding_matrix[word2id[\"<PAD>\"]] = np.zeros(embedding_dim)\n",
    "    \n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float)"
   ],
   "id": "f84fc6823b629e0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ОБУЧЕНИЕ",
   "id": "2dee636ed1f16add"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Загрузка данных",
   "id": "fb7efa016b605e29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dataset = NerDataset(TRAIN_BIO_PATH)\n",
    "\n",
    "val_dataset = NerDataset(VAL_BIO_PATH, \n",
    "                         word2id=train_dataset.word2id, \n",
    "                         char2id=train_dataset.char2id, \n",
    "                         tag2id=train_dataset.tag2id)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "print(\"\\nИнформация: Датасеты и даталоадеры успешно созданы.\")\n",
    "\n",
    "embedding_weights = load_and_prepare_embeddings(train_dataset.word2id, EMBEDDINGS_PATH, WORD_EMBEDDING_DIM)"
   ],
   "id": "723c0986ded598b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Инициализация модели",
   "id": "891480b1966f41bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if embedding_weights is not None:\n",
    "    model = BiLSTMCrfForNer(\n",
    "        word_vocab_size=len(train_dataset.word2id),\n",
    "        word_embedding_dim=WORD_EMBEDDING_DIM,\n",
    "        char_vocab_size=len(train_dataset.char2id),\n",
    "        char_embedding_dim=CHAR_EMBEDDING_DIM,\n",
    "        char_hidden_dim=CHAR_HIDDEN_DIM,\n",
    "        lstm_hidden_dim=LSTM_HIDDEN_DIM,\n",
    "        num_tags=len(train_dataset.tag2id),\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        padding_idx=train_dataset.word2id[\"<PAD>\"]\n",
    "    )\n",
    "    \n",
    "    model.word_embedding.weight.data.copy_(embedding_weights)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    print(\"\\nИнформация: Модель и оптимизатор инициализированы.\")"
   ],
   "id": "a6f5830eac67f53a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Запуск обучения",
   "id": "bd123e707458fe3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "best_val_f1 = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(\"\\n--- Начало процесса обучения ---\")\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer)\n",
    "    \n",
    "    report = eval_epoch(model, val_dataloader, train_dataset.id2tag)\n",
    "    val_f1_macro = report['macro avg']['f1-score']\n",
    "    \n",
    "    print(f\"\\nЭпоха {epoch}/{NUM_EPOCHS}:\")\n",
    "    print(f\"  Потери на обучении (Train Loss): {train_loss:.4f}\")\n",
    "    print(f\"  F1-macro на валидации: {val_f1_macro:.4f}\")\n",
    "\n",
    "    print(\"  Детальный отчет по классам:\")\n",
    "    for tag, metrics in report.items():\n",
    "        if isinstance(metrics, dict):\n",
    "            print(f\"    - {tag:<10}: F1={metrics['f1-score']:.4f}, Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}\")\n",
    "\n",
    "    if val_f1_macro > best_val_f1:\n",
    "        best_val_f1 = val_f1_macro\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        print(f\"  Новый лучший результат! Модель сохранена (F1-macro: {best_val_f1:.4f}).\")\n",
    "\n",
    "print(\"\\n--- Обучение завершено ---\")"
   ],
   "id": "ee3d1f8bda91b5dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Сохрнение результатов",
   "id": "9aed5e46fdf29dce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "torch.save(best_model_state, os.path.join(MODELS_DIR, \"bilstm_v1.pth\"))\n",
    "\n",
    "artefacts = {\n",
    "    \"word2id\": train_dataset.word2id,\n",
    "    \"char2id\": train_dataset.char2id,\n",
    "    \"tag2id\": train_dataset.tag2id,\n",
    "    \"id2tag\": train_dataset.id2tag\n",
    "}\n",
    "with open(os.path.join(MODELS_DIR, \"artefacts_v1.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(artefacts, f)\n",
    "\n",
    "print(f\"\\nИнформация: Лучшая модель и артефакты сохранены в директорию: {MODELS_DIR}\")"
   ],
   "id": "468d340b8b26261"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
