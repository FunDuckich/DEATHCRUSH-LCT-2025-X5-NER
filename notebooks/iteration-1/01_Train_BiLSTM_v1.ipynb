{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T16:56:09.292695Z",
     "start_time": "2025-09-20T16:56:06.692095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from TorchCRF import CRF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import ast\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "TRAIN_BIO_PATH = \"../../data/processed/train_bio.csv\"\n",
    "VAL_BIO_PATH = \"../../data/processed/validation_bio.csv\"\n",
    "EMBEDDINGS_PATH = \"../../data/external/embeddings/ru_en_aligned.pkl\"\n",
    "MODELS_DIR = \"../../models/iteration-1/\""
   ],
   "id": "51122954c3d90f50",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T16:56:09.308216Z",
     "start_time": "2025-09-20T16:56:09.297718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Информация: Вычисления будут производиться на устройстве: {device}\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"Информация: Random seed ({SEED}) установлен для обеспечения воспроизводимости.\")\n",
    "\n",
    "print(\"\\nДиагностика CUDA:\")\n",
    "print(f\"- torch.cuda.is_available(): {torch.cuda.is_available()}\")\n",
    "print(f\"- torch.version.cuda: {getattr(torch.version, 'cuda', None)}\")\n",
    "try:\n",
    "    from torch.backends import cudnn\n",
    "    print(f\"- cudnn.enabled: {cudnn.enabled}, cudnn.version(): {cudnn.version() if hasattr(cudnn, 'version') else None}\")\n",
    "except Exception as e:\n",
    "    print(f\"- Информация по cuDNN недоступна: {e}\")\n",
    "print(f\"- torch.cuda.device_count(): {torch.cuda.device_count()}\")\n",
    "for idx in range(torch.cuda.device_count()):\n",
    "    try:\n",
    "        print(f\"  * [{idx}] {torch.cuda.get_device_name(idx)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  * [{idx}] ошибка чтения имени устройства: {e}\")\n"
   ],
   "id": "bbf6ec690b0523e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Информация: Вычисления будут производиться на устройстве: cpu\n",
      "Информация: Random seed (42) установлен для обеспечения воспроизводимости.\n",
      "\n",
      "Диагностика CUDA:\n",
      "- torch.cuda.is_available(): False\n",
      "- torch.version.cuda: None\n",
      "- cudnn.enabled: True, cudnn.version(): None\n",
      "- torch.cuda.device_count(): 0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Гиперпараметры ",
   "id": "dc347489880f7b04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T16:56:09.320658Z",
     "start_time": "2025-09-20T16:56:09.317634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "WORD_EMBEDDING_DIM = 300 \n",
    "CHAR_EMBEDDING_DIM = 50  \n",
    "CHAR_HIDDEN_DIM = 50      \n",
    "\n",
    "LSTM_HIDDEN_DIM = 256\n",
    "LSTM_NUM_LAYERS = 2 \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 10 \n",
    "DROPOUT_RATE = 0.5 "
   ],
   "id": "a40eca17c0cc021f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Создание модели",
   "id": "bf9254404624ce99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T16:56:09.339725Z",
     "start_time": "2025-09-20T16:56:09.333694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CharEmbedding(nn.Module):\n",
    "    def __init__(self, char_vocab_size, embedding_dim, hidden_dim, dropout_rate=0.25):\n",
    "        super(CharEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(char_vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        print(\"Информация: Модуль CharEmbedding успешно инициализирован.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, word_len = x.size()\n",
    "        \n",
    "        x = x.view(batch_size * seq_len, word_len)\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        \n",
    "        output = lstm_out.permute(0, 2, 1)\n",
    "        output = torch.max(output, 2)[0]\n",
    "        \n",
    "        output = output.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        return self.dropout(output)"
   ],
   "id": "c53e827ffdce5bda",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T16:56:09.359227Z",
     "start_time": "2025-09-20T16:56:09.352211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BiLSTMCrfForNer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 word_vocab_size,\n",
    "                 word_embedding_dim,\n",
    "                 char_vocab_size,\n",
    "                 char_embedding_dim,\n",
    "                 char_hidden_dim,\n",
    "                 lstm_hidden_dim,\n",
    "                 num_tags,\n",
    "                 dropout_rate=0.33,\n",
    "                 padding_idx=0):\n",
    "        super(BiLSTMCrfForNer, self).__init__()\n",
    "\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=word_vocab_size,\n",
    "            embedding_dim=word_embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        self.word_embedding.weight.requires_grad = False\n",
    "\n",
    "        self.char_embedding = CharEmbedding(\n",
    "            char_vocab_size=char_vocab_size,\n",
    "            embedding_dim=char_embedding_dim,\n",
    "            hidden_dim=char_hidden_dim,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.embedding_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=word_embedding_dim + (2 * char_hidden_dim),\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if 2 > 1 else 0\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(2 * lstm_hidden_dim, num_tags)\n",
    "\n",
    "        self.crf = CRF(num_tags=num_tags, batch_first=True)\n",
    "        \n",
    "        print(\"Информация: Основная модель BiLSTMCrfForNer успешно инициализирована.\")\n",
    "\n",
    "    def forward(self, word_ids, char_ids, mask, tags=None):\n",
    "        # word_ids: (batch_size, seq_len)\n",
    "        # char_ids: (batch_size, seq_len, word_len)\n",
    "        # mask: (batch_size, seq_len)\n",
    "        # tags: (batch_size, seq_len)\n",
    "\n",
    "        word_embeds = self.word_embedding(word_ids)\n",
    "        char_embeds = self.char_embedding(char_ids)\n",
    "        \n",
    "        combined_embeds = torch.cat([word_embeds, char_embeds], dim=-1)\n",
    "        combined_embeds = self.embedding_dropout(combined_embeds)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(combined_embeds)\n",
    "        \n",
    "        emissions = self.classifier(lstm_out)\n",
    "\n",
    "        # Гарантируем булев тип маски для совместимости с PyTorch и CRF\n",
    "        mask = mask.bool()\n",
    "\n",
    "        if tags is not None:\n",
    "            loss = -self.crf(emissions, tags, mask=mask, reduction='mean')\n",
    "            return loss\n",
    "        else:\n",
    "            decoded_tags = self.crf.decode(emissions, mask=mask)\n",
    "            return decoded_tags"
   ],
   "id": "1a1edc3e6e787f9e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Подача данных",
   "id": "2565014e528d16e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T16:56:09.384749Z",
     "start_time": "2025-09-20T16:56:09.370303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NerDataset(Dataset):\n",
    "    def __init__(self, df_path, word2id=None, char2id=None, tag2id=None):\n",
    "        self.df = pd.read_csv(df_path, sep=\";\")\n",
    "        \n",
    "        self.df['tokens'] = self.df['tokens'].apply(ast.literal_eval)\n",
    "        self.df['tags'] = self.df['tags'].apply(ast.literal_eval)\n",
    "\n",
    "        if word2id is None:\n",
    "            self.word2id, self.id2word = self._build_vocab(self.df['tokens'])\n",
    "        else:\n",
    "            self.word2id, self.id2word = word2id, {v: k for k, v in word2id.items()}\n",
    "\n",
    "        if char2id is None:\n",
    "            all_chars = set(\"\".join([\"\".join(tokens) for tokens in self.df['tokens']]))\n",
    "            self.char2id, self.id2char = self._build_char_vocab(all_chars)\n",
    "        else:\n",
    "            self.char2id, self.id2char = char2id, {v: k for k, v in char2id.items()}\n",
    "\n",
    "        if tag2id is None:\n",
    "            # ВАЖНО: строим словарь ТЕГОВ без <UNK>, обязательно добавляем O\n",
    "            self.tag2id, self.id2tag = self._build_tag_vocab(self.df['tags'])\n",
    "        else:\n",
    "            self.tag2id, self.id2tag = tag2id, {v: k for k, v in tag2id.items()}\n",
    "            \n",
    "        print(f\"Информация: Dataset загружен. Размер: {len(self.df)} записей.\")\n",
    "        print(f\"Информация: Размер словаря слов: {len(self.word2id)}\")\n",
    "        print(f\"Информация: Размер словаря символов: {len(self.char2id)}\")\n",
    "        print(f\"Информация: Размер словаря тегов: {len(self.tag2id)}\")\n",
    "\n",
    "    def _build_vocab(self, data):\n",
    "        vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        for sequence in data:\n",
    "            for item in sequence:\n",
    "                if item not in vocab:\n",
    "                    vocab[item] = len(vocab)\n",
    "        id2vocab = {v: k for k, v in vocab.items()}\n",
    "        return vocab, id2vocab\n",
    "\n",
    "    def _build_char_vocab(self, chars):\n",
    "        vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        for char in sorted(list(chars)):\n",
    "            if char not in vocab:\n",
    "                vocab[char] = len(vocab)\n",
    "        id2vocab = {v: k for k, v in vocab.items()}\n",
    "        return vocab, id2vocab\n",
    "\n",
    "    # НОВОЕ: билдер словаря для тегов (без <UNK>), с обязательным 'O'\n",
    "    def _build_tag_vocab(self, tags_series):\n",
    "        tag_vocab = {\"<PAD>\": 0}\n",
    "        # гарантируем наличие 'O'\n",
    "        if \"O\" not in tag_vocab:\n",
    "            tag_vocab[\"O\"] = len(tag_vocab)\n",
    "        for seq in tags_series:\n",
    "            for tag in seq:\n",
    "                if tag not in tag_vocab:\n",
    "                    tag_vocab[tag] = len(tag_vocab)\n",
    "        id2tag = {v: k for k, v in tag_vocab.items()}\n",
    "        return tag_vocab, id2tag\n",
    "    # ... existing code ...\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        tokens = row['tokens']\n",
    "        tags = row['tags']\n",
    "\n",
    "        word_ids = [self.word2id.get(token, self.word2id[\"<UNK>\"]) for token in tokens]\n",
    "        # ВАЖНО: неизвестные теги (на случай аномалий) отправляем в 'O', а не в <PAD>\n",
    "        o_idx = self.tag2id.get(\"O\", 0)\n",
    "        tag_ids = [self.tag2id.get(tag, o_idx) for tag in tags]  # никаких паддингов в середине последовательности\n",
    "        \n",
    "        char_ids = []\n",
    "        for token in tokens:\n",
    "            ids = [self.char2id.get(char, self.char2id[\"<UNK>\"]) for char in token]\n",
    "            char_ids.append(ids)\n",
    "\n",
    "        return {\"words\": word_ids, \"chars\": char_ids, \"tags\": tag_ids}\n",
    "\n",
    "def collate_fn(batch, word_pad_idx=0, char_pad_idx=0, tag_pad_idx=0):\n",
    "    max_seq_len = max(len(item['words']) for item in batch)\n",
    "    max_word_len = max(max(len(char_seq) for char_seq in item['chars']) if item['chars'] else 0 for item in batch)\n",
    "\n",
    "    padded_words, padded_chars, padded_tags, masks = [], [], [], []\n",
    "\n",
    "    for item in batch:\n",
    "        seq_len = len(item['words'])\n",
    "        \n",
    "        padded_words.append(item['words'] + [word_pad_idx] * (max_seq_len - seq_len))\n",
    "        padded_tags.append(item['tags'] + [tag_pad_idx] * (max_seq_len - seq_len))\n",
    "        \n",
    "        masks.append([1] * seq_len + [0] * (max_seq_len - seq_len))\n",
    "        \n",
    "        padded_char_seq = []\n",
    "        for char_seq in item['chars']:\n",
    "            padded_char_seq.append(char_seq + [char_pad_idx] * (max_word_len - len(char_seq)))\n",
    "        \n",
    "        if seq_len < max_seq_len:\n",
    "            for _ in range(max_seq_len - seq_len):\n",
    "                padded_char_seq.append([char_pad_idx] * max_word_len)\n",
    "        \n",
    "        padded_chars.append(padded_char_seq)\n",
    "\n",
    "    return {\n",
    "        \"words\": torch.tensor(padded_words, dtype=torch.long),\n",
    "        \"chars\": torch.tensor(padded_chars, dtype=torch.long),\n",
    "        \"tags\": torch.tensor(padded_tags, dtype=torch.long),\n",
    "        \"mask\": torch.tensor(masks, dtype=torch.bool)\n",
    "    }"
   ],
   "id": "1c34813e294d9b1b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Цикл обучения и оценки",
   "id": "646479da5e7ee2c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T16:56:09.410704Z",
     "start_time": "2025-09-20T16:56:09.393323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_class_weights(tags_series, tag2id):\n",
    "    all_tags = [tag for seq in tags_series for tag in seq]\n",
    "    tag_counts = Counter(all_tags)\n",
    "    \n",
    "    # Создаем веса. Используем сглаживание, чтобы избежать деления на ноль.\n",
    "    # Более редкие классы получат больший вес.\n",
    "    weights = torch.ones(len(tag2id), device=device)\n",
    "    for tag, count in tag_counts.items():\n",
    "        if tag in tag2id:\n",
    "            # Вес обратно пропорционален частоте\n",
    "            weights[tag2id[tag]] = 1.0 / (count + 1e-6) \n",
    "    \n",
    "    # Нормализуем веса\n",
    "    weights = weights / weights.sum()\n",
    "    # Увеличим вес редких классов еще сильнее\n",
    "    weights = weights.pow(0.5)\n",
    "\n",
    "    print(\"Информация: Рассчитаны веса для классов:\")\n",
    "    for tag, i in tag2id.items():\n",
    "        print(f\"- {tag}: {weights[i]:.4f}\")\n",
    "        \n",
    "    return weights\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    # Прогресс-бар по батчам\n",
    "    pbar = tqdm(dataloader, desc=\"Train\", leave=False, dynamic_ncols=True)\n",
    "    for batch in pbar:\n",
    "        # Перенос данных на нужное устройство\n",
    "        words = batch['words'].to(device)\n",
    "        chars = batch['chars'].to(device)\n",
    "        tags = batch['tags'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "\n",
    "        # Обнуление градиентов\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Прямой проход и вычисление потерь\n",
    "        loss = model(words, chars, mask, tags)\n",
    "        \n",
    "        # Обратный проход\n",
    "        loss.backward()\n",
    "        \n",
    "        # Шаг оптимизатора\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        # Обновляем подпись прогресс-бара\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# --- НОВОЕ: Entity-level оценка BIO ---\n",
    "\n",
    "def _extract_entities_bio(tags_seq):\n",
    "    \"\"\"\n",
    "    Превращает список BIO-тегов в множество сущностей вида (type, start, end),\n",
    "    где start/end — индексы (включительно) в пределах последовательности.\n",
    "    Нормализация BIO: одиночные 'I-X' без валидного 'B-X' считаем как начало новой сущности.\n",
    "    Служебные теги ('O', '<PAD>', '<UNK>') игнорируются.\n",
    "    \"\"\"\n",
    "    entities = set()\n",
    "    cur_type = None\n",
    "    start = None\n",
    "\n",
    "    def close_entity(end_idx):\n",
    "        nonlocal cur_type, start\n",
    "        if cur_type is not None and start is not None:\n",
    "            entities.add((cur_type, start, end_idx))\n",
    "        cur_type, start = None, None\n",
    "\n",
    "    for i, tag in enumerate(tags_seq):\n",
    "        if tag in (\"O\", \"<PAD>\", \"<UNK>\") or not isinstance(tag, str):\n",
    "            # Закрываем текущую сущность\n",
    "            if cur_type is not None:\n",
    "                close_entity(i - 1)\n",
    "            continue\n",
    "\n",
    "        if tag.startswith(\"B-\"):\n",
    "            # Закрываем предыдущую, открываем новую\n",
    "            if cur_type is not None:\n",
    "                close_entity(i - 1)\n",
    "            cur_type = tag[2:]\n",
    "            start = i\n",
    "        elif tag.startswith(\"I-\"):\n",
    "            t = tag[2:]\n",
    "            if cur_type == t and start is not None:\n",
    "                # продолжаем ту же сущность\n",
    "                pass\n",
    "            else:\n",
    "                # нарушение BIO: начинаем новую сущность\n",
    "                if cur_type is not None:\n",
    "                    close_entity(i - 1)\n",
    "                cur_type = t\n",
    "                start = i\n",
    "        else:\n",
    "            # Неверный/неизвестный формат — закрываем текущую\n",
    "            if cur_type is not None:\n",
    "                close_entity(i - 1)\n",
    "\n",
    "    # Закрываем хвост, если открыт\n",
    "    if cur_type is not None:\n",
    "        close_entity(len(tags_seq) - 1)\n",
    "\n",
    "    return entities\n",
    "\n",
    "def _compute_entity_report(all_true_entities, all_pred_entities):\n",
    "    \"\"\"\n",
    "    На вход — списки множеств сущностей по батчам/предложениям.\n",
    "    Возвращает отчёт с precision/recall/f1 per type и micro/macro/weighted averages.\n",
    "    \"\"\"\n",
    "    # Агрегируем по типам\n",
    "    per_type = {}\n",
    "    support_per_type = Counter()\n",
    "\n",
    "    total_tp = total_fp = total_fn = 0\n",
    "\n",
    "    for true_set, pred_set in zip(all_true_entities, all_pred_entities):\n",
    "        # По типам\n",
    "        types = set([t for (t, _, _) in true_set]) | set([t for (t, _, _) in pred_set])\n",
    "        for t in types:\n",
    "            true_t = {e for e in true_set if e[0] == t}\n",
    "            pred_t = {e for e in pred_set if e[0] == t}\n",
    "            tp = len(true_t & pred_t)\n",
    "            fp = len(pred_t - true_t)\n",
    "            fn = len(true_t - pred_t)\n",
    "\n",
    "            if t not in per_type:\n",
    "                per_type[t] = {\"tp\": 0, \"fp\": 0, \"fn\": 0}\n",
    "            per_type[t][\"tp\"] += tp\n",
    "            per_type[t][\"fp\"] += fp\n",
    "            per_type[t][\"fn\"] += fn\n",
    "            support_per_type[t] += len(true_t)\n",
    "\n",
    "            total_tp += tp\n",
    "            total_fp += fp\n",
    "            total_fn += fn\n",
    "\n",
    "    # Формируем метрики\n",
    "    report = {}\n",
    "    for t, c in per_type.items():\n",
    "        tp, fp, fn = c[\"tp\"], c[\"fp\"], c[\"fn\"]\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "        report[t] = {\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec,\n",
    "            \"f1-score\": f1,\n",
    "            \"support\": support_per_type[t],\n",
    "        }\n",
    "\n",
    "    # Micro\n",
    "    micro_prec = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
    "    micro_rec = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
    "    micro_f1 = 2 * micro_prec * micro_rec / (micro_prec + micro_rec) if (micro_prec + micro_rec) > 0 else 0.0\n",
    "    report[\"micro avg\"] = {\n",
    "        \"precision\": micro_prec,\n",
    "        \"recall\": micro_rec,\n",
    "        \"f1-score\": micro_f1,\n",
    "        \"support\": sum(support_per_type.values()),\n",
    "    }\n",
    "\n",
    "    # Macro\n",
    "    valid_types = [t for t in report.keys() if t not in (\"micro avg\", \"macro avg\", \"weighted avg\")]\n",
    "    if valid_types:\n",
    "        macro_prec = sum(report[t][\"precision\"] for t in valid_types) / len(valid_types)\n",
    "        macro_rec = sum(report[t][\"recall\"] for t in valid_types) / len(valid_types)\n",
    "        macro_f1 = sum(report[t][\"f1-score\"] for t in valid_types) / len(valid_types)\n",
    "    else:\n",
    "        macro_prec = macro_rec = macro_f1 = 0.0\n",
    "    report[\"macro avg\"] = {\n",
    "        \"precision\": macro_prec,\n",
    "        \"recall\": macro_rec,\n",
    "        \"f1-score\": macro_f1,\n",
    "        \"support\": sum(support_per_type.values()),\n",
    "    }\n",
    "\n",
    "    # Weighted\n",
    "    total_support = sum(support_per_type.values())\n",
    "    if total_support > 0:\n",
    "        weighted_prec = sum(report[t][\"precision\"] * support_per_type[t] for t in valid_types) / total_support\n",
    "        weighted_rec = sum(report[t][\"recall\"] * support_per_type[t] for t in valid_types) / total_support\n",
    "        weighted_f1 = sum(report[t][\"f1-score\"] * support_per_type[t] for t in valid_types) / total_support\n",
    "    else:\n",
    "        weighted_prec = weighted_rec = weighted_f1 = 0.0\n",
    "    report[\"weighted avg\"] = {\n",
    "        \"precision\": weighted_prec,\n",
    "        \"recall\": weighted_rec,\n",
    "        \"f1-score\": weighted_f1,\n",
    "        \"support\": total_support,\n",
    "    }\n",
    "\n",
    "    return report\n",
    "\n",
    "def eval_epoch_entities(model, dataloader, id2tag):\n",
    "    \"\"\"\n",
    "    Entity-level оценка: извлекаем сущности из BIO-последовательностей\n",
    "    и считаем метрики по сущностям.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_true_entities = []\n",
    "    all_pred_entities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Eval\", leave=False, dynamic_ncols=True):\n",
    "            words = batch['words'].to(device)\n",
    "            chars = batch['chars'].to(device)\n",
    "            tags = batch['tags'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            \n",
    "            # Получение предсказаний\n",
    "            predictions = model(words, chars, mask)\n",
    "            \n",
    "            # По каждому предложению вырезаем по фактической длине (по mask)\n",
    "            for i in range(len(predictions)):\n",
    "                seq_len = mask[i].sum().item()\n",
    "                true_ids = tags[i][:seq_len].cpu().tolist()\n",
    "                pred_ids = predictions[i][:seq_len]\n",
    "\n",
    "                true_tags = [id2tag[idx] for idx in true_ids]\n",
    "                pred_tags = [id2tag[idx] for idx in pred_ids]\n",
    "\n",
    "                true_ents = _extract_entities_bio(true_tags)\n",
    "                pred_ents = _extract_entities_bio(pred_tags)\n",
    "\n",
    "                all_true_entities.append(true_ents)\n",
    "                all_pred_entities.append(pred_ents)\n",
    "\n",
    "    # Готовим финальный отчёт\n",
    "    return _compute_entity_report(all_true_entities, all_pred_entities)"
   ],
   "id": "c47cb657604d7dd6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Подготовка эмбеддингов",
   "id": "99163b9c4443e94c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T16:56:09.423791Z",
     "start_time": "2025-09-20T16:56:09.417921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_and_prepare_embeddings(word2id, filepath, embedding_dim):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        fasttext_model = pickle.load(f)\n",
    "\n",
    "    embedding_matrix = np.random.uniform(-0.05, 0.05, (len(word2id), embedding_dim))\n",
    "    \n",
    "    hits = 0\n",
    "    for word, i in word2id.items():\n",
    "        if word in fasttext_model:\n",
    "            embedding_matrix[i] = fasttext_model[word]\n",
    "            hits += 1\n",
    "    \n",
    "    print(f\"Информация: Найдено {hits} из {len(word2id)} слов в предобученной модели ({hits / len(word2id) * 100:.2f}%).\")\n",
    "    \n",
    "    embedding_matrix[word2id[\"<PAD>\"]] = np.zeros(embedding_dim)\n",
    "    \n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float)"
   ],
   "id": "f84fc6823b629e0e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ОБУЧЕНИЕ",
   "id": "2dee636ed1f16add"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Загрузка данных",
   "id": "fb7efa016b605e29"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T16:56:27.453878Z",
     "start_time": "2025-09-20T16:56:09.435344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = NerDataset(TRAIN_BIO_PATH)\n",
    "\n",
    "val_dataset = NerDataset(VAL_BIO_PATH, \n",
    "                         word2id=train_dataset.word2id, \n",
    "                         char2id=train_dataset.char2id, \n",
    "                         tag2id=train_dataset.tag2id)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "print(\"\\nИнформация: Датасеты и даталоадеры успешно созданы.\")\n",
    "\n",
    "embedding_weights = load_and_prepare_embeddings(train_dataset.word2id, EMBEDDINGS_PATH, WORD_EMBEDDING_DIM)"
   ],
   "id": "723c0986ded598b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Информация: Dataset загружен. Размер: 23163 записей.\n",
      "Информация: Размер словаря слов: 6108\n",
      "Информация: Размер словаря символов: 101\n",
      "Информация: Размер словаря тегов: 9\n",
      "Информация: Dataset загружен. Размер: 4088 записей.\n",
      "Информация: Размер словаря слов: 6108\n",
      "Информация: Размер словаря символов: 101\n",
      "Информация: Размер словаря тегов: 9\n",
      "\n",
      "Информация: Датасеты и даталоадеры успешно созданы.\n",
      "Информация: Найдено 3132 из 6108 слов в предобученной модели (51.28%).\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Инициализация модели",
   "id": "891480b1966f41bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T16:56:29.176978Z",
     "start_time": "2025-09-20T16:56:27.620450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if embedding_weights is not None:\n",
    "    model = BiLSTMCrfForNer(\n",
    "        word_vocab_size=len(train_dataset.word2id),\n",
    "        word_embedding_dim=WORD_EMBEDDING_DIM,\n",
    "        char_vocab_size=len(train_dataset.char2id),\n",
    "        char_embedding_dim=CHAR_EMBEDDING_DIM,\n",
    "        char_hidden_dim=CHAR_HIDDEN_DIM,\n",
    "        lstm_hidden_dim=LSTM_HIDDEN_DIM,\n",
    "        num_tags=len(train_dataset.tag2id),\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        padding_idx=train_dataset.word2id[\"<PAD>\"]\n",
    "    )\n",
    "    \n",
    "    model.word_embedding.weight.data.copy_(embedding_weights)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    print(\"\\nИнформация: Модель и оптимизатор инициализированы.\")"
   ],
   "id": "a6f5830eac67f53a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Информация: Модуль CharEmbedding успешно инициализирован.\n",
      "Информация: Основная модель BiLSTMCrfForNer успешно инициализирована.\n",
      "\n",
      "Информация: Модель и оптимизатор инициализированы.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Оценка обучения",
   "id": "a38bab2a8919868f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def eval_epoch(model, dataloader, id2tag):\n",
    "    model.eval() \n",
    "    all_true_tags = []\n",
    "    all_pred_tags = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Оценка на валидации\"):\n",
    "            words = batch['words'].to(device)\n",
    "            chars = batch['chars'].to(device)\n",
    "            tags = batch['tags'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            \n",
    "            predictions = model(words, chars, mask)\n",
    "            \n",
    "            for i in range(len(predictions)):\n",
    "                seq_len = mask[i].sum().item()\n",
    "                \n",
    "                true_tags = tags[i][:seq_len].cpu().tolist()\n",
    "                pred_tags = predictions[i][:seq_len]\n",
    "                \n",
    "                all_true_tags.extend([id2tag[tag_id] for tag_id in true_tags])\n",
    "                all_pred_tags.extend([id2tag[tag_id] for tag_id in pred_tags])\n",
    "\n",
    "    labels_to_include = [tag for tag in id2tag.values() if tag not in [\"O\", \"<PAD>\", \"<UNK>\"]]\n",
    "    \n",
    "    report = classification_report(\n",
    "        all_true_tags, \n",
    "        all_pred_tags, \n",
    "        labels=labels_to_include,\n",
    "        output_dict=True, \n",
    "        zero_division=0 \n",
    "    )\n",
    "    \n",
    "    return report"
   ],
   "id": "2947a69e2bfb17ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Запуск обучения",
   "id": "bd123e707458fe3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T17:04:38.984082Z",
     "start_time": "2025-09-20T16:56:29.190580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_val_f1 = 0.0\n",
    "history = []\n",
    "\n",
    "print(\"\\n--- Начало процесса обучения ---\")\n",
    "# Прогресс-бар по эпохам\n",
    "for epoch in tqdm(range(1, NUM_EPOCHS + 1), desc=\"Epochs\", unit=\"epoch\", dynamic_ncols=True):\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer)\n",
    "    \n",
    "    # НОВОЕ: entity-level отчёт\n",
    "    report = eval_epoch_entities(model, val_dataloader, train_dataset.id2tag)\n",
    "    val_f1_macro = report['macro avg']['f1-score']\n",
    "    val_precision_macro = report['macro avg']['precision']\n",
    "    val_recall_macro = report['macro avg']['recall']\n",
    "    \n",
    "    history.append({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss,\n",
    "        'val_f1': val_f1_macro,\n",
    "        'val_precision': val_precision_macro,\n",
    "        'val_recall': val_recall_macro\n",
    "    })\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(f\"--- Эпоха {epoch}/{NUM_EPOCHS} --- (Длительность: {epoch_duration:.2f} сек)\")\n",
    "    print(f\"  Потери на обучении (Train Loss): {train_loss:.4f}\")\n",
    "    print(f\"  F1-macro (entity-level) на валидации: {val_f1_macro:.4f}\")\n",
    "\n",
    "    print(\"  Детальный отчёт по сущностям:\")\n",
    "    # Выводим только реальные типы (без агрегатов)\n",
    "    for tag, metrics in report.items():\n",
    "        if isinstance(metrics, dict) and tag not in {\"micro avg\", \"macro avg\", \"weighted avg\"}:\n",
    "            print(f\"    - {tag:<10}: F1={metrics['f1-score']:.4f}, Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, Support={metrics['support']}\")\n",
    "\n",
    "    print(\"  Агрегаты:\")\n",
    "    for agg in (\"micro avg\", \"macro avg\", \"weighted avg\"):\n",
    "        m = report[agg]\n",
    "        print(f\"    - {agg:<12}: F1={m['f1-score']:.4f}, Precision={m['precision']:.4f}, Recall={m['recall']:.4f}, Support={m['support']}\")\n",
    "\n",
    "    if val_f1_macro > best_val_f1:\n",
    "        best_val_f1 = val_f1_macro\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        print(f\"  Новый лучший результат! Модель сохранена (F1-macro entity-level: {best_val_f1:.4f}).\")\n",
    "        os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "        torch.save(best_model_state, os.path.join(MODELS_DIR, \"bilstm_v1_best.pth\"))\n",
    "\n",
    "print(\"\\n--- Обучение завершено ---\")\n",
    "print(f\"Лучший F1-macro на валидации: {best_val_f1:.4f}\")"
   ],
   "id": "ee3d1f8bda91b5dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Начало процесса обучения ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?epoch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a080e771aef64b3fb9b326b5a7b5319f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Train:   0%|          | 0/724 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35e959d576e44877a0b0a1b3f705e192"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Eval:   0%|          | 0/128 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38683f68f1374bdaa6f4167dd7114247"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Эпоха 1/10:\n",
      "  Потери на обучении (Train Loss): 1.7776\n",
      "  F1-macro (entity-level) на валидации: 0.4001\n",
      "  Детальный отчёт по сущностям:\n",
      "    - TYPE      : F1=0.9239, Precision=0.8801, Recall=0.9723, Support=4372\n",
      "    - BRAND     : F1=0.6765, Precision=0.8115, Recall=0.5801, Support=1143\n",
      "    - VOLUME    : F1=0.0000, Precision=0.0000, Recall=0.0000, Support=17\n",
      "    - PERCENT   : F1=0.0000, Precision=0.0000, Recall=0.0000, Support=1\n",
      "  Агрегаты:\n",
      "    - micro avg   : F1=0.8791, Precision=0.8702, Recall=0.8881, Support=5533\n",
      "    - macro avg   : F1=0.4001, Precision=0.4229, Recall=0.3881, Support=5533\n",
      "    - weighted avg: F1=0.8698, Precision=0.8631, Recall=0.8881, Support=5533\n",
      "  Новый лучший результат! Модель сохранена (F1-macro entity-level: 0.4001).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train:   0%|          | 0/724 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79a6417dfa2643e58a89f3f29820b16c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Eval:   0%|          | 0/128 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23b073f78f5d4d4498024fffe3f0c908"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Эпоха 2/10:\n",
      "  Потери на обучении (Train Loss): 0.9985\n",
      "  F1-macro (entity-level) на валидации: 0.4027\n",
      "  Детальный отчёт по сущностям:\n",
      "    - TYPE      : F1=0.9264, Precision=0.8800, Recall=0.9780, Support=4372\n",
      "    - BRAND     : F1=0.6842, Precision=0.8607, Recall=0.5678, Support=1143\n",
      "    - VOLUME    : F1=0.0000, Precision=0.0000, Recall=0.0000, Support=17\n",
      "    - PERCENT   : F1=0.0000, Precision=0.0000, Recall=0.0000, Support=1\n",
      "  Агрегаты:\n",
      "    - micro avg   : F1=0.8837, Precision=0.8774, Recall=0.8901, Support=5533\n",
      "    - macro avg   : F1=0.4027, Precision=0.4352, Recall=0.3865, Support=5533\n",
      "    - weighted avg: F1=0.8734, Precision=0.8732, Recall=0.8901, Support=5533\n",
      "  Новый лучший результат! Модель сохранена (F1-macro entity-level: 0.4027).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train:   0%|          | 0/724 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e5d0c3f726c486b853e735fddf7f940"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Eval:   0%|          | 0/128 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32ecdbda894b4bb0a24e10b094f6a0c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Эпоха 3/10:\n",
      "  Потери на обучении (Train Loss): 0.7785\n",
      "  F1-macro (entity-level) на валидации: 0.4178\n",
      "  Детальный отчёт по сущностям:\n",
      "    - TYPE      : F1=0.9302, Precision=0.9056, Recall=0.9563, Support=4372\n",
      "    - BRAND     : F1=0.7408, Precision=0.7669, Recall=0.7165, Support=1143\n",
      "    - VOLUME    : F1=0.0000, Precision=0.0000, Recall=0.0000, Support=17\n",
      "    - PERCENT   : F1=0.0000, Precision=0.0000, Recall=0.0000, Support=1\n",
      "  Агрегаты:\n",
      "    - micro avg   : F1=0.8914, Precision=0.8795, Recall=0.9037, Support=5533\n",
      "    - macro avg   : F1=0.4178, Precision=0.4181, Recall=0.4182, Support=5533\n",
      "    - weighted avg: F1=0.8881, Precision=0.8740, Recall=0.9037, Support=5533\n",
      "  Новый лучший результат! Модель сохранена (F1-macro entity-level: 0.4178).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train:   0%|          | 0/724 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e69b5a2862ec4beeaf85d777bf0c8d2f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Eval:   0%|          | 0/128 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82ecb71c5fde4c3793744a8d4db4e8ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Эпоха 4/10:\n",
      "  Потери на обучении (Train Loss): 0.6605\n",
      "  F1-macro (entity-level) на валидации: 0.4863\n",
      "  Детальный отчёт по сущностям:\n",
      "    - TYPE      : F1=0.9313, Precision=0.8933, Recall=0.9728, Support=4372\n",
      "    - BRAND     : F1=0.7412, Precision=0.8213, Recall=0.6754, Support=1143\n",
      "    - VOLUME    : F1=0.2727, Precision=0.6000, Recall=0.1765, Support=17\n",
      "    - PERCENT   : F1=0.0000, Precision=0.0000, Recall=0.0000, Support=1\n",
      "  Агрегаты:\n",
      "    - micro avg   : F1=0.8946, Precision=0.8809, Recall=0.9087, Support=5533\n",
      "    - macro avg   : F1=0.4863, Precision=0.5786, Recall=0.4562, Support=5533\n",
      "    - weighted avg: F1=0.8899, Precision=0.8774, Recall=0.9087, Support=5533\n",
      "  Новый лучший результат! Модель сохранена (F1-macro entity-level: 0.4863).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train:   0%|          | 0/724 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6d6a547c23d464dbcb255c5d45c0d82"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Eval:   0%|          | 0/128 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8955def1d670435fba6da38727576299"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Эпоха 5/10:\n",
      "  Потери на обучении (Train Loss): 0.5903\n",
      "  F1-macro (entity-level) на валидации: 0.5277\n",
      "  Детальный отчёт по сущностям:\n",
      "    - TYPE      : F1=0.9373, Precision=0.9017, Recall=0.9758, Support=4372\n",
      "    - BRAND     : F1=0.7598, Precision=0.8529, Recall=0.6850, Support=1143\n",
      "    - VOLUME    : F1=0.4138, Precision=0.5000, Recall=0.3529, Support=17\n",
      "    - PERCENT   : F1=0.0000, Precision=0.0000, Recall=0.0000, Support=1\n",
      "  Агрегаты:\n",
      "    - micro avg   : F1=0.9032, Precision=0.8930, Recall=0.9136, Support=5533\n",
      "    - macro avg   : F1=0.5277, Precision=0.5637, Recall=0.5034, Support=5533\n",
      "    - weighted avg: F1=0.8988, Precision=0.8902, Recall=0.9136, Support=5533\n",
      "  Новый лучший результат! Модель сохранена (F1-macro entity-level: 0.5277).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train:   0%|          | 0/724 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73c6620edf54486a815d6e5e244e2b94"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Eval:   0%|          | 0/128 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "726413d2aa70433c86180d576783d5d6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Эпоха 6/10:\n",
      "  Потери на обучении (Train Loss): 0.5357\n",
      "  F1-macro (entity-level) на валидации: 0.5221\n",
      "  Детальный отчёт по сущностям:\n",
      "    - TYPE      : F1=0.9362, Precision=0.8978, Recall=0.9780, Support=4372\n",
      "    - BRAND     : F1=0.7524, Precision=0.8833, Recall=0.6553, Support=1143\n",
      "    - PERCENT   : F1=0.0800, Precision=0.0417, Recall=1.0000, Support=1\n",
      "    - VOLUME    : F1=0.3200, Precision=0.5000, Recall=0.2353, Support=17\n",
      "  Агрегаты:\n",
      "    - micro avg   : F1=0.9001, Precision=0.8914, Recall=0.9091, Support=5533\n",
      "    - macro avg   : F1=0.5221, Precision=0.5807, Recall=0.7172, Support=5533\n",
      "    - weighted avg: F1=0.8962, Precision=0.8934, Recall=0.9091, Support=5533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train:   0%|          | 0/724 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "81ec1c83ea824d1f987958be759ae6e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Eval:   0%|          | 0/128 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a80a7b9674114312a13961f7ed2efbac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Эпоха 7/10:\n",
      "  Потери на обучении (Train Loss): 0.5003\n",
      "  F1-macro (entity-level) на валидации: 0.5908\n",
      "  Детальный отчёт по сущностям:\n",
      "    - TYPE      : F1=0.9413, Precision=0.9110, Recall=0.9737, Support=4372\n",
      "    - BRAND     : F1=0.7876, Precision=0.8666, Recall=0.7218, Support=1143\n",
      "    - VOLUME    : F1=0.6341, Precision=0.5417, Recall=0.7647, Support=17\n",
      "    - PERCENT   : F1=0.0000, Precision=0.0000, Recall=0.0000, Support=1\n",
      "  Агрегаты:\n",
      "    - micro avg   : F1=0.9110, Precision=0.9015, Recall=0.9208, Support=5533\n",
      "    - macro avg   : F1=0.5908, Precision=0.5798, Recall=0.6150, Support=5533\n",
      "    - weighted avg: F1=0.9084, Precision=0.9005, Recall=0.9208, Support=5533\n",
      "  Новый лучший результат! Модель сохранена (F1-macro entity-level: 0.5908).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train:   0%|          | 0/724 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f5f8c800b81b491d9b3e9e534ec585ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Eval:   0%|          | 0/128 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7cde2f74232d479e9d354f000508cfa1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Эпоха 8/10:\n",
      "  Потери на обучении (Train Loss): 0.4664\n",
      "  F1-macro (entity-level) на валидации: 0.6200\n",
      "  Детальный отчёт по сущностям:\n",
      "    - TYPE      : F1=0.9430, Precision=0.9156, Recall=0.9721, Support=4372\n",
      "    - BRAND     : F1=0.7962, Precision=0.8786, Recall=0.7279, Support=1143\n",
      "    - VOLUME    : F1=0.7407, Precision=1.0000, Recall=0.5882, Support=17\n",
      "    - PERCENT   : F1=0.0000, Precision=0.0000, Recall=0.0000, Support=1\n",
      "  Агрегаты:\n",
      "    - micro avg   : F1=0.9148, Precision=0.9094, Recall=0.9203, Support=5533\n",
      "    - macro avg   : F1=0.6200, Precision=0.6985, Recall=0.5721, Support=5533\n",
      "    - weighted avg: F1=0.9119, Precision=0.9080, Recall=0.9203, Support=5533\n",
      "  Новый лучший результат! Модель сохранена (F1-macro entity-level: 0.6200).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train:   0%|          | 0/724 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e108f465f6247e59d380644412b85b5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Eval:   0%|          | 0/128 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8aaf000b2e754c0c8cac35789ee05716"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Эпоха 9/10:\n",
      "  Потери на обучении (Train Loss): 0.4369\n",
      "  F1-macro (entity-level) на валидации: 0.5385\n",
      "  Детальный отчёт по сущностям:\n",
      "    - TYPE      : F1=0.9477, Precision=0.9240, Recall=0.9728, Support=4372\n",
      "    - BRAND     : F1=0.8252, Precision=0.8777, Recall=0.7787, Support=1143\n",
      "    - VOLUME    : F1=0.3810, Precision=1.0000, Recall=0.2353, Support=17\n",
      "    - PERCENT   : F1=0.0000, Precision=0.0000, Recall=0.0000, Support=1\n",
      "  Агрегаты:\n",
      "    - micro avg   : F1=0.9227, Precision=0.9153, Recall=0.9302, Support=5533\n",
      "    - macro avg   : F1=0.5385, Precision=0.7004, Recall=0.4967, Support=5533\n",
      "    - weighted avg: F1=0.9205, Precision=0.9145, Recall=0.9302, Support=5533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train:   0%|          | 0/724 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f33d0110281c461b86e08258fd008162"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Eval:   0%|          | 0/128 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "838d9617935a4d2bb8b182eebe4079fc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Эпоха 10/10:\n",
      "  Потери на обучении (Train Loss): 0.4117\n",
      "  F1-macro (entity-level) на валидации: 0.5319\n",
      "  Детальный отчёт по сущностям:\n",
      "    - TYPE      : F1=0.9500, Precision=0.9309, Recall=0.9700, Support=4372\n",
      "    - BRAND     : F1=0.8329, Precision=0.8418, Recall=0.8241, Support=1143\n",
      "    - VOLUME    : F1=0.3448, Precision=0.4167, Recall=0.2941, Support=17\n",
      "    - PERCENT   : F1=0.0000, Precision=0.0000, Recall=0.0000, Support=1\n",
      "  Агрегаты:\n",
      "    - micro avg   : F1=0.9240, Precision=0.9107, Recall=0.9376, Support=5533\n",
      "    - macro avg   : F1=0.5319, Precision=0.5473, Recall=0.5221, Support=5533\n",
      "    - weighted avg: F1=0.9238, Precision=0.9107, Recall=0.9376, Support=5533\n",
      "\n",
      "--- Обучение завершено ---\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Сохранение результатов",
   "id": "9aed5e46fdf29dce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T17:04:39.077603Z",
     "start_time": "2025-09-20T17:04:39.044310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "artefacts = {\n",
    "    \"word2id\": train_dataset.word2id,\n",
    "    \"char2id\": train_dataset.char2id,\n",
    "    \"tag2id\": train_dataset.tag2id,\n",
    "    \"id2tag\": train_dataset.id2tag\n",
    "}\n",
    "\n",
    "with open(ARTEFACTS_PATH, \"wb\") as f:\n",
    "    pickle.dump(artefacts, f)\n",
    "\n",
    "print(f\"Информация: Словари для инференса (артефакты) успешно сохранены в: {ARTEFACTS_PATH}\")\n",
    "\n",
    "\n",
    "history_df = pd.DataFrame(history)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 6)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.plot(history_df['epoch'], history_df['train_loss'], label='Потери на обучении', color='b', marker='o')\n",
    "ax1.set_title('Динамика потерь на обучении по эпохам', fontsize=14)\n",
    "ax1.set_xlabel('Эпоха', fontsize=12)\n",
    "ax1.set_ylabel('Значение Loss', fontsize=12)\n",
    "ax1.legend()\n",
    "ax1.xaxis.set_major_locator(plt.MaxNLocator(integer=True)) \n",
    "\n",
    "\n",
    "ax2.plot(history_df['epoch'], history_df['val_f1'], label='F1-macro', color='g', marker='o')\n",
    "ax2.plot(history_df['epoch'], history_df['val_precision'], label='Precision-macro', color='r', marker='s', linestyle='--')\n",
    "ax2.plot(history_df['epoch'], history_df['val_recall'], label='Recall-macro', color='orange', marker='^', linestyle='--')\n",
    "ax2.set_title('Динамика метрик на валидации по эпохам', fontsize=14)\n",
    "ax2.set_xlabel('Эпоха', fontsize=12)\n",
    "ax2.set_ylabel('Значение метрики', fontsize=12)\n",
    "ax2.legend()\n",
    "ax2.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "\n",
    "plt.savefig(CHART_PATH)\n",
    "\n",
    "print(f\"Информация: Графики процесса обучения сохранены в: {CHART_PATH}\")\n",
    "\n",
    "plt.show()"
   ],
   "id": "468d340b8b26261",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Информация: Лучшая модель и артефакты сохранены в директорию: ../../models/iteration-1/\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T17:04:39.129725Z",
     "start_time": "2025-09-20T17:04:39.126670Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5f2b2eb379d3a39f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
