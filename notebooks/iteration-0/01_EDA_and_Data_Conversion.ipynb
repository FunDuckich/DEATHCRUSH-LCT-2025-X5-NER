{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import fasttext\n",
    "import umap\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "tokens_by_class = {\n",
    "    'TYPE': set(),\n",
    "    'BRAND': set(),\n",
    "    'VOLUME': set(),\n",
    "    'PERCENT': set()\n",
    "}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Настройки для визуализаций\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"viridis\")\n",
    "plt.rcParams['figure.figsize'] = (12, 7)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Константы для путей к файлам\n",
    "DATA_RAW_PATH = '../../data/raw/'\n",
    "TRAIN_FILE = DATA_RAW_PATH + 'train.csv'\n",
    "SUBMISSION_FILE = DATA_RAW_PATH + 'submission.csv'"
   ],
   "id": "1442a54998714973"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_train = pd.read_csv(TRAIN_FILE, sep=';')\n",
    "df_submission = pd.read_csv(SUBMISSION_FILE, sep=';')\n",
    "\n",
    "print(\"Обучающий набор данных (train.csv):\")\n",
    "df_train.info()\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"Тестовый набор данных (submission.csv):\")\n",
    "df_submission.info()\n",
    "print(\"\\n\")"
   ],
   "id": "781227409a7036e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Тип данных колонки 'annotation' до преобразования:\", df_train['annotation'].dtype)\n",
    "\n",
    "# Применяем безопасный парсинг\n",
    "df_train['annotation'] = df_train['annotation'].apply(ast.literal_eval)\n",
    "\n",
    "print(\"Тип данных колонки 'annotation' после преобразования:\", df_train['annotation'].dtype)\n",
    "print(\"\\nПример данных в колонке 'annotation' после преобразования:\")\n",
    "print(df_train['annotation'].iloc[5])"
   ],
   "id": "14a943e02324103c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Анализ длины запросов",
   "id": "27b923d948552eeb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_train['query_len_char'] = df_train['sample'].str.len()\n",
    "df_train['query_len_token'] = df_train['sample'].str.split().str.len()\n",
    "\n",
    "df_submission['query_len_char'] = df_submission['sample'].str.len()\n",
    "df_submission['query_len_token'] = df_submission['sample'].str.split().str.len()\n",
    "\n",
    "print(\"Статистика по длинам запросов в обучающем наборе:\")\n",
    "display(df_train[['query_len_char', 'query_len_token']].describe())\n",
    "\n",
    "print(\"\\nСтатистика по длинам запросов в тестовом наборе:\")\n",
    "display(df_submission[['query_len_char', 'query_len_token']].describe())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.histplot(df_train['query_len_char'], color='blue', label='Train', kde=True, stat=\"density\", linewidth=0)\n",
    "sns.histplot(df_submission['query_len_char'], color='red', label='Submission', kde=True, stat=\"density\", linewidth=0, alpha=0.6)\n",
    "plt.title('Распределение длин запросов в символах (Train vs Submission)')\n",
    "plt.xlabel('Длина запроса в символах')\n",
    "plt.ylabel('Плотность')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.histplot(df_train['query_len_token'], color='blue', label='Train', kde=True, stat=\"density\", linewidth=0, binwidth=1)\n",
    "sns.histplot(df_submission['query_len_token'], color='red', label='Submission', kde=True, stat=\"density\", linewidth=0, alpha=0.6, binwidth=1)\n",
    "plt.title('Распределение длин запросов в токенах (Train vs Submission)')\n",
    "plt.xlabel('Длина запроса в токенах')\n",
    "plt.ylabel('Плотность')\n",
    "plt.legend()\n",
    "plt.xlim(0, 20)\n",
    "plt.xticks(range(0, 21))\n",
    "plt.show()"
   ],
   "id": "bb0d8791912c0bfa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Анализ мультиязычности",
   "id": "84460eb577728a0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def contains_latin(text):\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "    return any('a' <= char.lower() <= 'z' for char in text)"
   ],
   "id": "a2dea3280969b60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_train['contains_latin'] = df_train['sample'].apply(contains_latin)\n",
    "df_submission['contains_latin'] = df_submission['sample'].apply(contains_latin)\n",
    "\n",
    "train_latin_ratio = df_train['contains_latin'].mean()\n",
    "submission_latin_ratio = df_submission['contains_latin'].mean()\n",
    "\n",
    "print(f\"Доля запросов с латиницей в обучающем наборе: {train_latin_ratio:.2%}\")\n",
    "print(f\"Доля запросов с латиницей в тестовом наборе: {submission_latin_ratio:.2%}\")\n",
    "\n",
    "print(\"\\nПримеры запросов с латиницей из обучающего набора:\")\n",
    "display(df_train[df_train['contains_latin']].sample(10, random_state=42))"
   ],
   "id": "de67a00079a4f91c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Анализ баланса классов сущностей",
   "id": "9d748d81399d3f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_entity_type(tag):\n",
    "    if '-' in tag:\n",
    "        return tag.split('-')[1]\n",
    "    return tag"
   ],
   "id": "e8721cc44d394958"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "entity_counter = Counter()\n",
    "for annotation_list in df_train['annotation']:\n",
    "    for _, _, tag in annotation_list:\n",
    "        entity_type = get_entity_type(tag)\n",
    "        if entity_type != 'O':\n",
    "            entity_counter[entity_type] += 1\n",
    "\n",
    "df_entity_counts = pd.DataFrame(entity_counter.items(), columns=['Entity', 'Count']).sort_values('Count', ascending=False)\n",
    "\n",
    "print(\"Распределение сущностей в обучающем наборе:\")\n",
    "display(df_entity_counts)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Entity', y='Count', data=df_entity_counts)\n",
    "plt.title('Распределение количества сущностей по типам')\n",
    "plt.xlabel('Тип сущности')\n",
    "plt.ylabel('Количество')\n",
    "for index, row in df_entity_counts.iterrows():\n",
    "    plt.text(row.name, row.Count, row.Count, color='black', ha=\"center\", va=\"bottom\")\n",
    "plt.show()"
   ],
   "id": "7e31ffd0f836eac8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Частотный анализ",
   "id": "2e4feb3c98a76aec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "all_tokens = df_train['sample'].dropna().str.lower().str.split().sum()\n",
    "\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "print(\"Топ-30 самых частотных слов в обучающем наборе:\")\n",
    "display(pd.DataFrame(token_counts.most_common(30), columns=['Токен', 'Частота']))"
   ],
   "id": "6e18e164f8ef06f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Анализ неоднозначности токенов",
   "id": "53f1fce991eb92e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "token_to_tags = {}\n",
    "\n",
    "for _, row in df_train.iterrows():\n",
    "    query = row['sample']\n",
    "    annotations = row['annotation']\n",
    "    \n",
    "    for start, end, tag in annotations:\n",
    "        token = query[start:end].lower()\n",
    "        entity_type = get_entity_type(tag) \n",
    "        \n",
    "        if token not in token_to_tags:\n",
    "            token_to_tags[token] = set()\n",
    "        token_to_tags[token].add(entity_type)\n",
    "\n",
    "ambiguous_tokens = {token: tags for token, tags in token_to_tags.items() if len(tags) > 1}\n",
    "\n",
    "df_ambiguous = pd.DataFrame(ambiguous_tokens.items(), columns=['Токен', 'Возможные теги'])\n",
    "\n",
    "print(f\"Найдено {len(df_ambiguous)} неоднозначных токенов (могут иметь разные теги в разных контекстах).\")\n",
    "print(\"\\nПримеры 20 самых частотных неоднозначных токенов:\")\n",
    "\n",
    "df_ambiguous['Частота'] = df_ambiguous['Токен'].apply(lambda x: token_counts.get(x, 0))\n",
    "df_ambiguous = df_ambiguous.sort_values('Частота', ascending=False)\n",
    "\n",
    "display(df_ambiguous.head(20).reset_index(drop=True))"
   ],
   "id": "8716e7db1f849119"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Анализ контекстного окружения сущностей",
   "id": "bcb4d94e58698a1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "context_before = {entity: Counter() for entity in ['TYPE', 'BRAND', 'VOLUME', 'PERCENT']}\n",
    "context_after = {entity: Counter() for entity in ['TYPE', 'BRAND', 'VOLUME', 'PERCENT']}\n",
    "\n",
    "for _, row in df_train.iterrows():\n",
    "    query_tokens = str(row['sample']).lower().split()\n",
    "    \n",
    "    current_pos = 0\n",
    "    token_tags = ['O'] * len(query_tokens)\n",
    "    token_indices = []\n",
    "    for token in query_tokens:\n",
    "        start = row['sample'].lower().find(token, current_pos)\n",
    "        end = start + len(token)\n",
    "        token_indices.append((start, end))\n",
    "        current_pos = end\n",
    "\n",
    "    for start_ann, end_ann, tag_ann in row['annotation']:\n",
    "        entity_type = get_entity_type(tag_ann)\n",
    "        if entity_type == 'O':\n",
    "            continue\n",
    "            \n",
    "        for i, (start_tok, end_tok) in enumerate(token_indices):\n",
    "            if max(start_ann, start_tok) < min(end_ann, end_tok):\n",
    "                \n",
    "                if i > 0:\n",
    "                    prev_token = query_tokens[i-1]\n",
    "                    context_before[entity_type][prev_token] += 1\n",
    "                \n",
    "                if i < len(query_tokens) - 1:\n",
    "                    next_token = query_tokens[i+1]\n",
    "                    context_after[entity_type][next_token] += 1\n",
    "\n",
    "for entity_type in ['TYPE', 'BRAND', 'VOLUME', 'PERCENT']:\n",
    "    print(f\"\\n===== Контекст для сущности: {entity_type} =====\")\n",
    "    \n",
    "    df_before = pd.DataFrame(context_before[entity_type].most_common(10), columns=['Слово до', 'Частота'])\n",
    "    df_after = pd.DataFrame(context_after[entity_type].most_common(10), columns=['Слово после', 'Частота'])\n",
    "    \n",
    "    print(\"Топ-10 слов, встречающихся ДО сущности:\")\n",
    "    display(df_before)\n",
    "    \n",
    "    print(\"\\nТоп-10 слов, встречающихся ПОСЛЕ сущности:\")\n",
    "    display(df_after)"
   ],
   "id": "4df5351b14fe9f9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Анализ пространства признаков",
   "id": "fd283bf703f0f244"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for _, row in df_train.iterrows():\n",
    "    query = row['sample']\n",
    "    annotations = row['annotation']\n",
    "    \n",
    "    for start, end, tag in annotations:\n",
    "        entity_type = get_entity_type(tag)\n",
    "        if entity_type in tokens_by_class:\n",
    "            token = query[start:end].lower()\n",
    "            tokens_by_class[entity_type].add(token)\n",
    "\n",
    "all_unique_tokens = []\n",
    "labels = []\n",
    "for entity_type, token_set in tokens_by_class.items():\n",
    "    tokens_list = list(token_set)\n",
    "    all_unique_tokens.extend(tokens_list)\n",
    "    labels.extend([entity_type] * len(tokens_list))\n",
    "\n",
    "print(\"Подготовка данных завершена.\")\n",
    "for entity_type, token_set in tokens_by_class.items():\n",
    "    print(f\"Найдено {len(token_set)} уникальных токенов для класса '{entity_type}'\")"
   ],
   "id": "4cf2a77cedfab633"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Загрузка модели FastText... Это может занять несколько минут.\")\n",
    "ft_model_path = '../../models/external/cc.ru.300.bin'  # TODO Укажите свой путь к модели\n",
    "ft_model = fasttext.load_model(ft_model_path)\n",
    "print(\"Модель FastText успешно загружена.\")\n",
    "\n",
    "ft_embeddings = np.array([ft_model.get_word_vector(token) for token in all_unique_tokens])\n",
    "\n",
    "print(\"Применение UMAP для FastText эмбеддингов...\")\n",
    "reducer_ft = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "embedding_2d_ft = reducer_ft.fit_transform(ft_embeddings)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "df_plot_ft = pd.DataFrame({'x': embedding_2d_ft[:, 0], 'y': embedding_2d_ft[:, 1], 'label': labels})\n",
    "sns.scatterplot(data=df_plot_ft, x='x', y='y', hue='label', s=20, alpha=0.7)\n",
    "plt.title('Визуализация эмбеддингов токенов (FastText + UMAP)')\n",
    "plt.xlabel('UMAP компонента 1')\n",
    "plt.ylabel('UMAP компонента 2')\n",
    "plt.legend(title='Тип сущности')\n",
    "plt.show()"
   ],
   "id": "9dd9e6980c0f2fb8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Векторизация TF-IDF и n-gramms",
   "id": "71299cbd92aafe6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Векторизация с помощью TF-IDF на n-граммах символов...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "tfidf_embeddings = tfidf_vectorizer.fit_transform(all_unique_tokens)\n",
    "\n",
    "tfidf_embeddings_normalized = normalize(tfidf_embeddings)\n",
    "\n",
    "\n",
    "print(\"Применение UMAP для TF-IDF эмбеддингов...\")\n",
    "reducer_tfidf = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42, metric='hellinger')\n",
    "embedding_2d_tfidf = reducer_tfidf.fit_transform(tfidf_embeddings_normalized)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "df_plot_tfidf = pd.DataFrame({'x': embedding_2d_tfidf[:, 0], 'y': embedding_2d_tfidf[:, 1], 'label': labels})\n",
    "sns.scatterplot(data=df_plot_tfidf, x='x', y='y', hue='label', s=20, alpha=0.7)\n",
    "plt.title('Визуализация токенов (TF-IDF на n-граммах символов + UMAP)')\n",
    "plt.xlabel('UMAP компонента 1')\n",
    "plt.ylabel('UMAP компонента 2')\n",
    "plt.legend(title='Тип сущности')\n",
    "plt.show()"
   ],
   "id": "6cc24e5138340f35"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
